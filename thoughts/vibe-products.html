<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Is it the time of "Vibe Products"?</title>
    <link rel="stylesheet" href="../css/styles.css">
</head>
<body>
    <main class="thoughts-container">
        <article class="thought-post">
            <header>
                <h1>Is it the time of "Vibe Products"?</h1>
                <time datetime="2025-03-04">March 4, 2025</time>
            </header>
            
            <div class="thought-content">
                <p>LLMs, Agents and everything AI is pretty hard to evaluate. For one, the results of LLMs are not always deterministic. In addition, there are so many different use cases to evaluate these on, and in some of these the quality of the results is quite subjective anyway. This has led us to "vibe" checks. The famous Andrej Karpathy is thought to be the first to use this term in the context of AI. Now, when new LLMs are released, folks run a few of their favorite use cases to vibe check the results. You can find plenty of these vibe results on X and LinkedIn.</p>

                <p>Now folks started talking about "vibe coding": building a piece of software using some AI tool, often with some vague guidelines and most of the time the goal being getting something that "works" - where "works" is very, very loosely defined. I mean, it is all good fun, and some pretty cool stuff has come out of this.</p>

                <p>But, things can get more serious. There is a ton of AI or "AI" applications out there, in all verticals, cybersecurity included, that claim to perform some function F using AI. It is all great, but it is not quite clear how well they are actually implementing F. We all know that cybersecurity is not exactly rife with products with very clear and transparent ways to evaluate their results and as we all say, there is plenty of snake oil out there.</p>

                <p>Remember the good old pre-AI days? At least back then there was deterministic logic doing the work. And that work was either building some logic to collect and analyze data or some detection logic to find "bad" stuff. Most of the detections were rule-based so at least it was easy to review. Limited coverage? Sure. Too many false positives? You bet. But at least the logic was out there, so I (if I was a developer working in the company building the product) could verify it works. I could write a unittest or integration test to make sure that things don't break. Life was boring but predictable.</p>

                <p>But now in the age of AI, all this logic has been thrown to some LLM and anything can come back, often including the expected result (how generous!). Now it's quite hard to write a unittest or integration test since the results may be slightly different each time. Outsourcing multiple steps of logic to an LLM may result in rare but catastrophic failures. The point is that all these AI-based products are <strong>really</strong> hard to test and validate.</p>

                <p>So now I'm thinking: given the rate of announcements for AI-based security products and given how easy it is to build a prototype (and how hard it is to test it into a product), how many of these products have been tested to the (admittedly low in cybersecurity) standards of pre-AI products? Are these just demos being sold for the price of a real (tested) product?</p>

                <p>I am afraid we may have just entered the age of vibe products. Buckle up!</p>
            </div>
        </article>
    </main>
</body>
</html> 