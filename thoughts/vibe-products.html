<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Is it the time of "Vibe Products"?</title>
    <link rel="stylesheet" href="../css/styles.css">
</head>
<body>
    <main class="thoughts-container">
        <article class="thought-post">
            <header>
                <h1>Is it the time of "Vibe Products"?</h1>
                <time datetime="2025-03-04">March 4, 2025</time>
            </header>
            
            <div class="thought-content">
                <p>LLMs, Agents and everything AI is pretty hard to evaluate. For one, the results of LLMs are not always deterministic. In addition, there are so many different use cases to evaluate these on, and in some of these the quality of the results is quite subjective anyway. This has led us to "vibe" checks. The famous Adrej Karpathy is thought to be the first to use this term in the context of AI. Now, when new LLMs are released, folks run few of their favorite use cases to vibe check the results. You can find plenty of these vibe results on X and LinkedIn.</p>

                <p>Now folks started talking about "vibe coding": building a piece of software use some AI tool, often with some vague guidelines and most of the times the goal being getting something that "works" where works is very very losely defined. I mean, it is all good fun, and some pretty cool stuff has come out of this.</p>

                <p>But, things can get more serious. There is a ton of AI or "AI" applications out there, in all verticals, cybersecurity included, that claim to perform some function F using AI. It is all great, but it is not quite clear how well they are actually implementing F. We all know that cybersecurity is not exactly rife with products with very clear and transparent ways to evaluate their results and as we all say, there is plenty of snake oil out there.</p>

                <h2>The Pre-AI Era</h2>
                <p>In pre-AI products, at least there was a deterministic logic doing the work. And that work was either bulding some logic to collect and analyze data or some detection logic to find "bad" stuff. Most of the detections were rule based so at least it was easy to review. Limited coverage, maybe too many false positives, but at least the logic was out there, so I (If I was a developer working in the company building the product) could verify it works. I could write a unittest or integration test to make sure that things do not break. Life was boring but predictable.</p>

                <h2>The AI Challenge</h2>
                <p>In the age of AI, all this logic has been thrown to some LLM and anything can come back, often including the expected result. Now it is quite hard to write a unittest or integration test since the results may be slightly different each time. Outsourcing multiple steps of logic to an LLM may result in rare but catastrophic failures. The point is that all these AI based products are <strong>really</strong> hard to test and valiadate.</p>

                <p>So now I am thinking: given the rate of announcmenets for AI based security products and given how easy it is to build a prorotype (and how hard it is to test it into a product), how many of these products have been tested to the (admitedly low in cybesecurity) standards of a pre-AI products. Are these just demos being sold for the price of a real (tested) product?</p>

                <p>I am afraid we may have just entered the age of vibe products.</p>
            </div>
        </article>
    </main>
</body>
</html> 